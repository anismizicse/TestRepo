#!/usr/bin/env python3
"""
Comprehensive API Validation with Exact Training Data Splits
Tests the API using the same data split methodology as training to get accurate performance metrics
"""

import sys
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import torch
from PIL import Image

# Add API directory to path
sys.path.insert(0, '/home/anis/Desktop/My_Files/kaggle/TestRepo/skin-type-api')
from model import load_model, predict

def create_df(base):
    """Create dataframe exactly as in training notebook"""
    label_index = {"dry": 0, "normal": 1, "oily": 2}
    dd = {"images": [], "labels": []}
    
    if not os.path.exists(base):
        print(f"âŒ Directory not found: {base}")
        return pd.DataFrame(dd)
    
    for i in os.listdir(base):
        if i in label_index:  # Include "dry", "normal", and "oily"
            label_dir = os.path.join(base, i)
            if os.path.exists(label_dir):
                for j in os.listdir(label_dir):
                    if j.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                        img_path = os.path.join(label_dir, j)
                        dd["images"].append(img_path)
                        dd["labels"].append(label_index[i])
    
    return pd.DataFrame(dd)

def validate_api_with_training_splits():
    """
    Validate API using EXACT same data splits as training
    This should give us the true performance comparison
    """
    
    print("ğŸ¯ COMPREHENSIVE API VALIDATION WITH TRAINING DATA SPLITS")
    print("=" * 80)
    
    # Step 1: Load API model
    print("1. Loading API model...")
    try:
        api_model, api_class_names = load_model()
        print(f"   âœ… API model loaded successfully")
        print(f"   ğŸ“‹ Classes: {api_class_names}")
    except Exception as e:
        print(f"   âŒ Failed to load API model: {e}")
        return False
    
    # Step 2: Create EXACT same data splits as training notebook
    print("\n2. Creating exact training data splits...")
    
    # Check for training data
    train_data_dir = "./Oily-Dry-Skin-Types/train"
    if not os.path.exists(train_data_dir):
        print(f"   âŒ Training data not found: {train_data_dir}")
        print("   ğŸ’¡ Make sure Oily-Dry-Skin-Types dataset is available")
        return False
    
    # Create dataframes exactly as in training notebook
    train_df = create_df("./Oily-Dry-Skin-Types/train")
    
    if len(train_df) == 0:
        print("   âŒ No training data found")
        return False
    
    print(f"   ğŸ“Š Total training images loaded: {len(train_df)}")
    
    # Apply EXACT same train_test_split as training notebook (from Cell 5)
    # Line: train_df, testing = train_test_split(train_df, random_state=42, test_size=0.2)
    # Line: val_df, test_df = train_test_split(testing, random_state=42, test_size=0.5)
    train_df_split, testing_split = train_test_split(train_df, random_state=42, test_size=0.2)
    val_df_split, test_df_split = train_test_split(testing_split, random_state=42, test_size=0.5)
    
    print(f"   âœ… Data splits created:")
    print(f"      - Training: {len(train_df_split)} images")
    print(f"      - Validation: {len(val_df_split)} images")
    print(f"      - Test: {len(test_df_split)} images")
    
    # Step 3: Test on validation split (this is what achieved 93% in training)
    print("\n3. Testing API on VALIDATION split (matches training performance)...")
    
    val_accuracy = test_api_on_split(api_model, api_class_names, val_df_split, "VALIDATION")
    
    # Step 4: Test on test split
    print("\n4. Testing API on TEST split...")
    
    test_accuracy = test_api_on_split(api_model, api_class_names, test_df_split, "TEST")
    
    # Step 5: Performance analysis
    print("\n5. COMPREHENSIVE PERFORMANCE ANALYSIS")
    print("=" * 60)
    
    print(f"ğŸ“Š Expected Training Validation Accuracy: 93.0%")
    print(f"ğŸ“Š API Validation Accuracy:              {val_accuracy*100:.1f}%")
    print(f"ğŸ“Š API Test Accuracy:                    {test_accuracy*100:.1f}%")
    
    val_diff = abs(val_accuracy - 0.93)
    
    if val_diff <= 0.05:  # Within 5%
        print("âœ… EXCELLENT: API performance matches training expectations!")
        success = True
    elif val_diff <= 0.10:  # Within 10%
        print("âœ… GOOD: API performance is close to training expectations")
        success = True
    else:
        print("âš ï¸ MODERATE: API performance differs from training")
        success = False
    
    print(f"\nğŸ’¡ Performance difference: {val_diff*100:.1f} percentage points")
    
    return success

def test_api_on_split(model, class_names, df_split, split_name):
    """Test API on a specific data split"""
    
    if len(df_split) == 0:
        print(f"   âŒ No data in {split_name} split")
        return 0.0
    
    # Test on subset for speed (or all if small dataset)
    test_count = min(100, len(df_split))
    test_data = df_split.head(test_count) if test_count < len(df_split) else df_split
    
    print(f"   ğŸ”„ Testing on {len(test_data)} {split_name.lower()} images...")
    
    true_labels = []
    predicted_labels = []
    confidences = []
    errors = 0
    
    for idx, row in test_data.iterrows():
        image_path = row['images']
        true_label_idx = row['labels']
        true_label = class_names[true_label_idx]
        
        try:
            # Load image and predict
            image = Image.open(image_path).convert("RGB")
            result = predict(model, image, class_names)
            
            predicted_label = result['top_class']
            confidence = result['predictions'][0]['confidence']
            
            true_labels.append(true_label)
            predicted_labels.append(predicted_label)
            confidences.append(confidence)
            
            # Progress indicator
            if predicted_label == true_label:
                print("âœ“", end="", flush=True)
            else:
                print("âœ—", end="", flush=True)
                
        except Exception as e:
            print("E", end="", flush=True)
            errors += 1
            continue
    
    print(f"  ({len(true_labels)} successful, {errors} errors)")
    
    if len(true_labels) == 0:
        print(f"   âŒ No successful predictions on {split_name} split")
        return 0.0
    
    # Calculate metrics
    accuracy = accuracy_score(true_labels, predicted_labels)
    avg_confidence = np.mean(confidences)
    
    print(f"   ğŸ“Š {split_name} Results:")
    print(f"      - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"      - Avg Confidence: {avg_confidence:.3f}")
    print(f"      - Successful predictions: {len(true_labels)}/{test_count}")
    
    # Detailed classification report
    print(f"\n   ğŸ“‹ Detailed {split_name} Classification Report:")
    print(classification_report(true_labels, predicted_labels, 
                              target_names=class_names, digits=3))
    
    # Per-class accuracy
    print(f"   ğŸ“ˆ Per-Class {split_name} Accuracy:")
    for class_name in class_names:
        class_mask = [tl == class_name for tl in true_labels]
        if any(class_mask):
            class_true = [tl for tl, mask in zip(true_labels, class_mask) if mask]
            class_pred = [pl for pl, mask in zip(predicted_labels, class_mask) if mask]
            class_acc = accuracy_score(class_true, class_pred)
            print(f"      - {class_name}: {class_acc:.3f} ({class_acc*100:.1f}%)")
    
    return accuracy

def compare_with_original_test_directory():
    """Compare performance on original test directory vs training splits"""
    
    print("\n6. COMPARISON: Training Splits vs Original Test Directory")
    print("=" * 70)
    
    # Load model
    model, class_names = load_model()
    
    # Test on original test directory
    test_dir = "./Oily-Dry-Skin-Types/test"
    if os.path.exists(test_dir):
        print("   ğŸ”„ Testing on original test directory...")
        
        original_test_results = []
        
        for class_name in class_names:
            class_dir = os.path.join(test_dir, class_name)
            if os.path.exists(class_dir):
                images = [f for f in os.listdir(class_dir) 
                         if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
                
                correct = 0
                total = 0
                
                # Test first 10 images of each class
                for img_file in images[:10]:
                    try:
                        img_path = os.path.join(class_dir, img_file)
                        image = Image.open(img_path).convert("RGB")
                        result = predict(model, image, class_names)
                        
                        if result['top_class'] == class_name:
                            correct += 1
                        total += 1
                        
                    except:
                        continue
                
                if total > 0:
                    acc = correct / total
                    original_test_results.append((class_name, correct, total, acc))
                    print(f"      - {class_name}: {correct}/{total} = {acc:.3f}")
        
        if original_test_results:
            total_correct = sum(r[1] for r in original_test_results)
            total_tested = sum(r[2] for r in original_test_results)
            overall_acc = total_correct / total_tested if total_tested > 0 else 0
            
            print(f"   ğŸ“Š Original Test Directory Accuracy: {overall_acc:.3f} ({overall_acc*100:.1f}%)")
            
            print("\n   ğŸ’¡ KEY INSIGHT:")
            print("      The difference in accuracy between training splits and original test")
            print("      directory is expected and normal. Training achieved 93% on validation")
            print("      split from training data, not on the separate original test directory.")
        
    else:
        print(f"   âš ï¸ Original test directory not found: {test_dir}")

if __name__ == "__main__":
    print("ğŸš€ Starting comprehensive API validation with training data splits...")
    
    # Change to correct directory
    os.chdir('/home/anis/Desktop/My_Files/kaggle/TestRepo')
    
    # Run validation
    success = validate_api_with_training_splits()
    
    # Compare with original test directory
    compare_with_original_test_directory()
    
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ VALIDATION SUCCESSFUL: API matches training performance!")
        print("ğŸ’¡ The API is working correctly and should achieve expected accuracy")
        print("   when deployed with data similar to the training distribution.")
    else:
        print("âš ï¸ VALIDATION NEEDS ATTENTION: Performance differs from training")
        print("ğŸ’¡ Consider retraining or checking for data preprocessing differences.")
    print("=" * 80)
